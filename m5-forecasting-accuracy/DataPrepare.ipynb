{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.path.append(str(current_dir.parent))\n",
    "from utils import get_competition_data_path, submit\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_dict = get_competition_data_path(\"m5-forecasting-accuracy\")\n",
    "competition_path = path_dict.get(\"competition_path\")\n",
    "train_path = path_dict.get(\"train_path\")\n",
    "submission_path = path_dict.get(\"sample_submission_path\")\n",
    "calendar_path = competition_path / \"calendar.csv\"\n",
    "sell_prices_path = competition_path / \"sell_prices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 14, 6\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props, skip=None):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024 ** 2\n",
    "    print(\"Memory usage of properties dataframe is :\", start_mem_usg, \" MB\")\n",
    "    NAlist = []  # Keeps track of columns that have missing values filled in.\n",
    "    for col in props.columns:\n",
    "        if skip is not None and col in skip:\n",
    "            continue\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "\n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \", col)\n",
    "            print(\"dtype before: \", props[col].dtype)\n",
    "\n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "\n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all():\n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn - 1, inplace=True)\n",
    "\n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = props[col] - asint\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)\n",
    "\n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "\n",
    "            # Print new column type\n",
    "            print(\"dtype after: \", props[col].dtype)\n",
    "            print(\"******************************\")\n",
    "\n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024 ** 2\n",
    "    print(\"Memory usage is: \", mem_usg, \" MB\")\n",
    "    print(\"This is \", 100 * mem_usg / start_mem_usg, \"% of the initial size\")\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The objective of the M5 forecasting competition is to advance the theory and practice of forecasting by identifying the method(s) that provide the most accurate point forecasts for each of the 42,840 time series of the competition. I addition, to elicit information to estimate the uncertainty distribution of the realized values of these series as precisely as possible. \n",
    "To that end, the participants of M5 are asked to provide 28 days ahead point forecasts (PFs) for all the series of the competition, as well as the corresponding median and 50%, 67%, 95%, and 99% prediction intervals (PIs).\n",
    "The M5 differs from the previous four ones in five important ways, some of them suggested by the discussants of the M4  competition, as follows:\n",
    "- First, it uses grouped unit sales data, starting at the product-store level and being aggregated to that of product departments, product categories, stores, and three geographical areas: the States of California (CA), Texas (TX), and Wisconsin (WI).\n",
    "- Second, besides the time series data, it includes explanatory variables such as sell prices, promotions, days of the week, and special events (e.g. Super Bowl, Valentineâ€™s Day, and Orthodox Easter) that typically affect unit sales and could improve forecasting accuracy.\n",
    "- Third, in addition to point forecasts, it assesses the distribution of uncertainty, as the participants are asked to provide information on nine indicative quantiles.\n",
    "- Fourth, instead of having a single competition to estimate both the point forecasts and the uncertainty distribution, there will be two parallel tracks using the same dataset, the first requiring 28 days ahead point forecasts and the second 28 days ahead probabilistic forecasts for the median and four prediction intervals (50%, 67%, 95%, and 99%).\n",
    "- Fifth, for the first time it focuses on series that display intermittency, i.e., sporadic demand including zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets info\n",
    "The M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = 20, 10\n",
    "\n",
    "img = mpimg.imread(\"../data/m5-forecasting-accuracy/m5-forecasting-hierarchical.png\")\n",
    "imgplot = plt.imshow(img)\n",
    "plt.grid(False)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outlier days: Xmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 14, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model choice\n",
    "- I decide to turn the time series problem into a supervised learning problem\n",
    "- Reasons:\n",
    "    - There's no existing package (that I know of) for time series analysis and forecast capable of dealing of this much amount of time series (statsmodels can only fit one series at a time)\n",
    "    - It's more straight forward to incorporate external features \n",
    "    - Random Forests and LightGBM are powerful algorithm that do really well in many types of problem, including time series forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No feature engineering\n",
    "- Use existing data\n",
    "- Drop features that I consider insignificant: event, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(calendar_path)\n",
    "calendar = calendar.drop(columns=['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(dtype='int')\n",
    "\n",
    "# Convert event to number\n",
    "all_event_names = np.append(calendar['event_name_1'].dropna().unique(), calendar['event_name_2'].dropna().unique()).reshape(-1, 1)\n",
    "all_event_types = np.append(calendar['event_type_1'].dropna().unique(), calendar['event_type_2'].dropna().unique()).reshape(-1, 1)\n",
    "\n",
    "name_codes = encoder.fit_transform(all_event_names) \n",
    "name_codes_dict = dict(zip(all_event_names.flatten().tolist(), name_codes.flatten().tolist()))\n",
    "\n",
    "type_codes = encoder.fit_transform(all_event_types) \n",
    "type_codes_dict = dict(zip(all_event_types.flatten().tolist(), type_codes.flatten().tolist()))\n",
    "\n",
    "# Change d to number\n",
    "d = calendar['d'].str.split('_', expand=True)[1].tolist()\n",
    "\n",
    "# Get quarter\n",
    "quarter = pd.to_datetime(calendar['date']).dt.quarter\n",
    "\n",
    "\n",
    "calendar = calendar.assign(d=d, quarter=quarter, event_name_1=event_name_1, event_type_1=event_type_1, event_name_2=event_name_2, event_type_2=event_type_2)\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales:\n",
    "- Changes days of sales to number\n",
    "- Turn to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(train_path)\n",
    "sample_submission = pd.read_csv(submission_path)\n",
    "\n",
    "outliers = ['d_331', 'd_697', 'd_1062', 'd_1427', 'd_1792']\n",
    "sales = sales.loc[:, ~sales.columns.isin(outliers)]\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn d into number\n",
    "d = sales.columns[6:].to_series().str.split('_', expand=True)[1].tolist()\n",
    "sales.columns = sales.columns[:6].tolist() + d\n",
    "\n",
    "# Keep only necessary columns\n",
    "sales = sales.drop(columns=['id'])\n",
    "\n",
    "# Use a subset of d \n",
    "if subset:\n",
    "    d = d[-subset:]\n",
    "sales = sales.reindex(columns=sales_cat_features + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode cat features to number\n",
    "item_id = encoder.fit_transform(sales[['item_id']]).astype(int)\n",
    "dept_id = encoder.fit_transform(sales[['dept_id']]).astype(int)\n",
    "cat_id = encoder.fit_transform(sales[['cat_id']]).astype(int)\n",
    "store_id = encoder.fit_transform(sales[['store_id']]).astype(int)\n",
    "state_id = encoder.fit_transform(sales[['state_id']]).astype(int)\n",
    "\n",
    "# Get item dict to map back later\n",
    "item_id_dict = dict(zip(sales['item_id'], item_id.flatten()))\n",
    "dept_id_dict = dict(zip(sales['dept_id'], dept_id.flatten()))\n",
    "cat_id_dict = dict(zip(sales['cat_id'], cat_id.flatten()))\n",
    "store_id_dict = dict(zip(sales['store_id'], store_id.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign number values\n",
    "sales = sales.assign(item_id=item_id, dept_id=dept_id, cat_id=cat_id, store_id=store_id, state_id=state_id)\n",
    "\n",
    "# To long format\n",
    "sales = sales.melt(id_vars=sales_cat_features, var_name='d', value_name='sale')\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach calendar and prices to sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with calendar\n",
    "sales = sales.merge(calendar, on='d')\n",
    "\n",
    "# Drop unused columns\n",
    "sales = sales.drop(columns=['wm_yr_wk', 'weekday', 'date'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar, sell_prices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales, _ = reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_feather('sales_1.feather')\n",
    "\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add features, specifically lagged values of sales\n",
    "    - Lag 1\n",
    "    - Lag 7\n",
    "    - Lag 28\n",
    "    - Lag 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_feather('sales_1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = sales.groupby(['item_id', 'store_id'])['sale']\n",
    "\n",
    "# Assign lag values\n",
    "sales['lag1'] = group.shift(1)\n",
    "sales['lag7'] = group.shift(7)\n",
    "sales['lag28'] = group.shift(28)\n",
    "sales['lag365'] = group.shift(365)\n",
    "\n",
    "# Test if the assignments work as intended\n",
    "test_item_id = 1437\n",
    "test_store_id = 0\n",
    "test_sale = sales.query('item_id == @test_item_id and store_id == @test_store_id')\n",
    "test_sale = test_sale.sort_values('d')\n",
    "np.testing.assert_allclose(test_sale['sale'].shift(7).to_numpy(), test_sale['lag7'].to_numpy())\n",
    "\n",
    "del group, test_item_id, test_store_id, test_sale\n",
    "\n",
    "sales = sales.dropna().reset_index(drop=True)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_feather('sales_2.feather')\n",
    "\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add more sales features: moving averages of lag. Idea: not only the last seasonal values affecting sales, but also a week and 4 weeks\n",
    "    - moving average of 7 for lag 1\n",
    "    - moving average of 28 for lag 1\n",
    "    - moving average of 7 for lag 7\n",
    "    - moving average of 28 for lag 7\n",
    "    - moving average of 7 for lag 28\n",
    "    - moving average of 28 for lag 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_feather('sales_2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_lag1 = sales.groupby(['item_id', 'store_id'])['lag1']\n",
    "group_lag7 = sales.groupby(['item_id', 'store_id'])['lag7']\n",
    "group_lag28 = sales.groupby(['item_id', 'store_id'])['lag28']\n",
    "\n",
    "# Assign ma values\n",
    "sales['ma7_lag1'] = group_lag1.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag1'] = group_lag1.transform(lambda x: x.rolling(28).mean())\n",
    "sales['ma7_lag7'] = group_lag7.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag7'] = group_lag7.transform(lambda x: x.rolling(28).mean())\n",
    "sales['ma7_lag28'] = group_lag28.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag28'] = group_lag28.transform(lambda x: x.rolling(28).mean())\n",
    "\n",
    "# Test if the assignments work as intended\n",
    "test_item_id = 1437\n",
    "test_store_id = 0\n",
    "test_sale = sales.query('item_id == @test_item_id and store_id == @test_store_id')\n",
    "test_sale = test_sale.sort_values('d')\n",
    "np.testing.assert_allclose(test_sale['lag7'].rolling(7).mean().to_numpy(), test_sale['ma7_lag7'].to_numpy())\n",
    "\n",
    "del group_lag7, group_lag28\n",
    "\n",
    "sales = sales.dropna().reset_index(drop=True)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_feather('sales_3.feather')\n",
    "\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All features\n",
    "- All the features above\n",
    "- Add event features\n",
    "    - event_name\n",
    "    - event_type\n",
    "- Add price features\n",
    "    - price\n",
    "    - changes in price\n",
    "    - mean of changes in price\n",
    "    - std of changes in price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(calendar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(dtype='int')\n",
    "\n",
    "# Convert event to number\n",
    "all_event_names = np.append(calendar['event_name_1'].dropna().unique(), calendar['event_name_2'].dropna().unique()).reshape(-1, 1)\n",
    "all_event_types = np.append(calendar['event_type_1'].dropna().unique(), calendar['event_type_2'].dropna().unique()).reshape(-1, 1)\n",
    "\n",
    "name_codes = encoder.fit_transform(all_event_names) \n",
    "name_codes_dict = dict(zip(all_event_names.flatten().tolist(), name_codes.flatten().tolist()))\n",
    "\n",
    "type_codes = encoder.fit_transform(all_event_types) \n",
    "type_codes_dict = dict(zip(all_event_types.flatten().tolist(), type_codes.flatten().tolist()))\n",
    "\n",
    "event_name_1 = calendar['event_name_1'].map(name_codes_dict).fillna(-1).astype('int')\n",
    "event_name_2 = calendar['event_name_2'].map(name_codes_dict).fillna(-1).astype('int')\n",
    "\n",
    "event_type_1 = calendar['event_type_1'].map(type_codes_dict).fillna(-1).astype('int')\n",
    "event_type_2 = calendar['event_type_2'].map(type_codes_dict).fillna(-1).astype('int')\n",
    "\n",
    "# Change d to number\n",
    "d = calendar['d'].str.split('_', expand=True)[1].tolist()\n",
    "\n",
    "# Get quarter\n",
    "quarter = pd.to_datetime(calendar['date']).dt.quarter\n",
    "\n",
    "\n",
    "calendar = calendar.assign(d=d, quarter=quarter, event_name_1=event_name_1, event_type_1=event_type_1, event_name_2=event_name_2, event_type_2=event_type_2)\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(train_path)\n",
    "sample_submission = pd.read_csv(submission_path)\n",
    "\n",
    "outliers = ['d_331', 'd_697', 'd_1062', 'd_1427', 'd_1792']\n",
    "sales = sales.loc[:, ~sales.columns.isin(outliers)]\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_cat_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn d into number\n",
    "d = sales.columns[6:].to_series().str.split('_', expand=True)[1].tolist()\n",
    "sales.columns = sales.columns[:6].tolist() + d\n",
    "\n",
    "# Keep only necessary columns\n",
    "sales = sales.drop(columns=['id'])\n",
    "\n",
    "# Use a subset of d \n",
    "if subset:\n",
    "    d = d[-subset:]\n",
    "sales = sales.reindex(columns=sales_cat_features + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode cat features to number\n",
    "item_id = encoder.fit_transform(sales[['item_id']]).astype(int)\n",
    "dept_id = encoder.fit_transform(sales[['dept_id']]).astype(int)\n",
    "cat_id = encoder.fit_transform(sales[['cat_id']]).astype(int)\n",
    "store_id = encoder.fit_transform(sales[['store_id']]).astype(int)\n",
    "state_id = encoder.fit_transform(sales[['state_id']]).astype(int)\n",
    "\n",
    "# Get item dict to map back later\n",
    "item_id_dict = dict(zip(sales['item_id'], item_id.flatten()))\n",
    "dept_id_dict = dict(zip(sales['dept_id'], dept_id.flatten()))\n",
    "cat_id_dict = dict(zip(sales['cat_id'], cat_id.flatten()))\n",
    "store_id_dict = dict(zip(sales['store_id'], store_id.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign number values\n",
    "sales = sales.assign(item_id=item_id, dept_id=dept_id, cat_id=cat_id, store_id=store_id, state_id=state_id)\n",
    "\n",
    "# To long format\n",
    "sales = sales.melt(id_vars=sales_cat_features, var_name='d', value_name='sale')\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach days to price and convert categorical columns to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices = pd.read_csv(sell_prices_path)\n",
    "sell_prices = sell_prices.assign(item_id=sell_prices['item_id'].map(item_id_dict), store_id=sell_prices['store_id'].map(store_id_dict))\n",
    "\n",
    "# Sort to make sure that diff is correct\n",
    "sell_prices = sell_prices.sort_values(['item_id', 'store_id', 'wm_yr_wk'])\n",
    "\n",
    "# Take diff to get change in price\n",
    "sell_prices['price_change'] = sell_prices.groupby(['item_id', 'store_id'])['sell_price'].diff().fillna(0)\n",
    "\n",
    "# Get mean and std of change in price\n",
    "group = sell_prices.groupby(['item_id', 'store_id'])['price_change']\n",
    "sell_prices['price_change_mean'] = group.transform(lambda x: x.mean())\n",
    "sell_prices['price_change_std'] = group.transform(lambda x: x.std())\n",
    "\n",
    "\n",
    "sell_prices = sell_prices.merge(calendar[['wm_yr_wk', 'd']], how='left').drop(columns='wm_yr_wk')\n",
    "\n",
    "del group\n",
    "gc.collect()\n",
    "\n",
    "sell_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach calendar and prices to sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with price\n",
    "sales = sales.merge(sell_prices, on=['d', 'item_id', 'store_id'], how='inner')\n",
    "\n",
    "# Merge with calendar\n",
    "sales = sales.merge(calendar, on='d')\n",
    "\n",
    "# Drop unused columns\n",
    "sales = sales.drop(columns=['wm_yr_wk', 'weekday', 'date'])\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar, sell_prices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales, _ = reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = sales.groupby(['item_id', 'store_id'])['sale']\n",
    "\n",
    "# Assign lag values\n",
    "sales['lag1'] = group.shift(1)\n",
    "sales['lag7'] = group.shift(7)\n",
    "sales['lag28'] = group.shift(28)\n",
    "sales['lag365'] = group.shift(365)\n",
    "\n",
    "\n",
    "group_lag1 = sales.groupby(['item_id', 'store_id'])['lag1']\n",
    "group_lag7 = sales.groupby(['item_id', 'store_id'])['lag7']\n",
    "group_lag28 = sales.groupby(['item_id', 'store_id'])['lag28']\n",
    "\n",
    "# Assign ma values\n",
    "sales['ma7_lag1'] = group_lag1.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag1'] = group_lag1.transform(lambda x: x.rolling(28).mean())\n",
    "sales['ma7_lag1'] = group_lag7.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma7_lag7'] = group_lag7.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag7'] = group_lag7.transform(lambda x: x.rolling(28).mean())\n",
    "sales['ma7_lag28'] = group_lag28.transform(lambda x: x.rolling(7).mean())\n",
    "sales['ma28_lag28'] = group_lag28.transform(lambda x: x.rolling(28).mean())\n",
    "\n",
    "sales = sales.dropna().reset_index(drop=True)\n",
    "\n",
    "del group, group_lag1, group_lag7, group_lag28\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_feather('sales_4.feather')\n",
    "\n",
    "del sales\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
